# Benchmark evaluation configuration

mode: evaluation

# Number of evaluation episodes
num_episodes: 50

# Task area randomization for evaluation (larger range than data collection)
# This allows testing the policy in a wider area to evaluate generalization
task_area_randomization:
  x: [-1.0, 1.0]      # ±0.4m in x direction (2x data collection range)
  y: [-1.0, 1.0]      # ±0.4m in y direction (2x data collection range)
  theta: [-0.524, 0.524]  # ±30 degrees (0.524 radians, 2x data collection range)

# Results directory
results_dir: data/benchmark/results

# Result naming: {task}_{scene}_{style}_{policytype}_{predictor}.json
save_format: "{task}_{scene}_{style}_{policy}_{predictor}.json"

# Note: horizon is read from JSON config (experiment.rollout.horizon or train.data[0].horizon)
# Note: render settings are controlled by robocasa.yaml (env.render)
