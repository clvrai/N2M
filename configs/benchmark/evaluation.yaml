# Benchmark evaluation configuration

mode: evaluation

# Number of evaluation episodes
num_episodes: 50

# Task area randomization for evaluation (larger range than data collection)
# This allows testing the policy in a wider area to evaluate generalization
task_area_randomization:
  x: [-2.0, 2.0]      # 2.0m in x direction 
  y: [-2.0, 2.0]      # 2.0m in y direction 
  theta: [-0.524, 0.524]  # Â±30 degrees (0.524 radians)

# Results directory
results_dir: data/benchmark/results

# Result naming: {task}_{scene}_{style}_{policytype}_{predictor}.json
save_format: "{task}_{scene}_{style}_{policy}_{predictor}.json"

# Note: horizon is read from JSON config (experiment.rollout.horizon or train.data[0].horizon)
# Note: render settings are controlled by robocasa.yaml (env.render)
